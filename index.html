
<!DOCTYPE html>
<html>
	<head>
		<title>Olivier Petit - PhD in Deep Learning and Medical Image Analaysis</title>
		<meta charset="utf-8" />
		<link rel="shortcut icon" href="https://www.olivier-petit.fr/favicon.ico">

		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
	</head>
	<body>


		<header>
			<div class="container">
				<nav class="navbar navbar-expand-md navbar-light">
						<a class="navbar-brand" href="/">Olivier Petit</a>
					<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapseContent" aria-controls="navbarCollapseContent" aria-expanded="false" aria-label="Toggle navigation">
						<span class="navbar-toggler-icon"></span>
					</button>
					<div class="collapse navbar-collapse" id="navbarCollapseContent">
						<ul class="navbar-nav ml-auto">
							<li class="nav-item">
								<a class="nav-link active" href="#introduction">Introduction</a>
							</li>
							<li class="nav-item">
								<a class="nav-link active" href="#activity">Activity</a>
							</li>
							<li class="nav-item">
								<a class="nav-link active" href="#publications">Publications</a>
							</li>
						</ul>
					</div>
				</nav>
			</div>
		</header>



		<main role="main">

			<div class="jumbotron jumbotron-fluid bg-light p-4">
				<div class="container">
					<div class="row">
					<div class="col-md-3 mt-2">
						<a href="/"><img class="rounded" src="images/avatar.jpg" alt="" style="width:150px" /></a>
					</div>
					<div class="col-md-6">
					<p class="lead my-4 mb-0">
					Hi! I'm <strong>Olivier Petit</strong>, PhD
					in Deep Learning and Computer Vision.
					</p>
					<p>
						<ul class="list-inline">
							<li class="list-inline-item"><a target="_blank" href="https://twitter.com/Olivier_Petit_H"><span class="fa fa-twitter" style="font-size: 1.5rem;"></span></a></li>
							<li class="list-inline-item"><a target="_blank" href="https://github.com/opetit"><span class="fa fa-github" style="font-size: 1.5rem;"></span></a></li>
							<li class="list-inline-item"><a target="_blank" href="https://www.linkedin.com/in/olivier-petit-97737a108/" style="font-size: 1.5rem;"><span class="fa fa-linkedin"></span></a></li>
							<li class="list-inline-item"><a target="_blank" href="mailto:contact@olivier-petit.fr"><span class="fa fa-envelope-o" style="font-size: 1.5rem;"></span></a></li>
						</ul>
					</p>
					</div>
					</div>
				</div>
			</div>

			<div class="container mt-4 mb-4">
				<h3 id="introduction">Introduction</h3>
				<hr>
				<p>After getting my MSc and engineering
				 degree in computer science and machine learning at the <a href="https://www.insa-rouen.fr/">INSA of Rouen</a>, I joined the
				 <a href="http://cedric.cnam.fr/">CEDRIC</a> at le CNAM in Paris to study <strong>Deep Learning</strong> and <strong>Computer Vision</strong> under
				the supervision of <a href="http://cedric.cnam.fr/~thomen/">Nicolas Thome</a>. I work directly with <a href="https://www.visiblepatient.com/">Visible Patient</a>
				which develop a software for professionals in the medical field. Thanks to that partnership I get my PhD degree in decembre 2021 in deep learning applied to medical images.
				I'm very proud and happy to bring <strong>Artificial Intelligence</strong> in medical applications.</p>

				
				<h3 class="pt-4" id="activity">Activity</h3>

				<ul>
				  <li>17/12/2021 : PhD defense at le CNAM</li>
				  <li>11/10/2021 : presentation of U-Transformer at the <a target="_blank" href="http://www.gdr-isis.fr/index.php/reunion/460/">GdR ISIS</a> day
				  <li>27/09/2021 : oral presentation of U-Transformer at MLMI, MICCAI 2021</li>
				  <li>07/09/2021 : accepted paper <b>3D Spatial Priors for Semi-Supervised Organ Segmentation with Deep Convolutional Neural Networks</b> at IJCARS
				  <li>08/08/2021 : accepted paper <b>U-Net Transformer: Self and Cross Attention for Medical Image Segmentation</b> at MLMI MICCAI 2021
				  <li>13/05/2021 : accepted paper <b>Iterative Confidence Relabeling with Deep ConvNets for Organ Segmentation with Partial Labels</b> at CMIG
				  <li>10/04/2019 : presentation of SMILE at the <a target="_blank" href="http://www.gdr-isis.fr/index.php?page=reunion&idreunion=386">GdR ISIS</a> day [<a href="/documents/SMILE_presentation_GDR_ISIS_2019.pdf">Slides</a>]</li>
				</ul>


				
				<h3 class="pt-4" id="publications">Publications</h3>
				<hr>
				
				
				<h5 class="mb-4">2021</h5>
				
				<div class="row">
					<div class="col-md-4">
						
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong></i>.<br> Thesis 
						"Semantic Segmentation of 3D Medical Images with Deep Learning"<br>
							[<a target="_blank" href='https://drive.google.com/file/d/1QvT9rzdITCWDFhyIbjxmZEDvGHUo1EgG/view?usp=sharing'>PDF</a>]
						</p>
					</div>
				</div>
				<div class="row">
					<div class="col-md-4">
						<img src="images/STIPPLE_550x180.png" class="img-fluid" alt="STIPPLE graphical absract">
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong>, Nicolas Thome and Luc Soler</i>.<br>
						"3D Spatial Priors for Semi-Supervised Organ Segmentation with Deep Convolutional Neural Networks"<br>
							<a target="_blank" href='https://www.springer.com/journal/11548/'>IJCARS 2021</a> [<a href='/documents/IJCARS_Prior_final.pdf'>PDF</a>]
						</p>
					</div>
					<div class="col-md-1">
						<button class="btn btn-secondary btn-sm" type="button" data-toggle="collapse" data-target="#abstractIJCARS_STIPPLE" aria-expanded="false" aria-controls="collapseExample">
							Abstract
						</button>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12 mt-4">
						<div class="collapse" id="abstractIJCARS_STIPPLE">
							<div class="card card-body mb-3">
								<p><strong>Purpose:</strong> Fully Convolutional neural Networks (FCNs) are the most popular models for medical image segmentation. However, they do not explicitly integrate spatial organ positions, which can be crucial for proper labeling in challenging contexts.</p>
								<p><strong>Methods:</strong> In this work, we propose a method that combines a model representing prior probabilities of an organ position in 3D with visual FCN predictions by means of a generalized prior-driven prediction function. The prior is also used in a self-labeling process to handle low-data regimes, in order to improve the quality of the pseudo-label selection.</p>
								<p><strong>Results:</strong> Experiments carried out on CT scans from the public TCIA pancreas segmentation dataset reveal that the resulting STIPPLE model can significantly increase performances compared to the FCN baseline, especially with few training images. We also show that STIPPLE outperforms state-of-the-art semi-supervised segmentation methods by leveraging the spatial prior information.</p>
								<p><strong>Conslusion:</strong> STIPPLE provides a segmentation method effective with few labeled examples, which is crucial in the medical domain. It offers an intuitive way to incorporate absolute position information by mimicking expert annotators.</p>
							</div>
						</div>
					</div>
				</div>
				
				
				
				<div class="row">
					<div class="col-md-4">
						<img src="images/U_Transformer_550x180.png" class="img-fluid" alt="U-Transformer graphical absract">
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong>, Nicolas Thome, Clement Rambour, Loic Themyr, Toby Collins and Luc Soler</i>.<br>
						"U-Net Transformer: Self and Cross Attention for Medical Image Segmentation"<br>
							<a target="_blank" href='https://miccai2021.org'>MICCAI 2021</a> workshop <a target="_blank" href='https://sites.google.com/view/mlmi2021/'>MLMI</a> [<a href='/documents/U_Transformer_MLMI_2021_final.pdf'>PDF</a>] [<a href='https://arxiv.org/abs/2103.06104'>arXiv</a>]
						</p>
					</div>
					<div class="col-md-1">
						<button class="btn btn-secondary btn-sm" type="button" data-toggle="collapse" data-target="#abstractU_Transformer" aria-expanded="false" aria-controls="collapseExample">
							Abstract
						</button>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12 mt-4">
						<div class="collapse" id="abstractU_Transformer">
							<div class="card card-body mb-3">
								Medical image segmentation remains particularly challenging for complex and low-contrast anatomical structures. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. U-Transformer overcomes the inability of U-Nets to model long-range contextual interactions and spatial dependencies, which are arguably crucial for accurate segmentation in challenging contexts. To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer.	
							</div>
						</div>
					</div>
				</div>

				<div class="row">
					<div class="col-md-4">
						<img src="images/INERRANT_550x180.png" class="img-fluid" alt="INERRANT graphical absract">
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong>, Nicolas Thome and Luc Soler</i>.<br>
						"Iterative Confidence Relabeling with Deep ConvNets for Organ Segmentation with Partial Labels"<br>
						<a target="_blank" href='https://doi.org/10.1016/j.compmedimag.2021.101938'>Computerized Medical Imaging and Graphics</a>, 2021 [<a href='/documents/INERRANT_CMIG_2021.pdf'>PDF</a>]
						</p>
					</div>
					<div class="col-md-1">
						<button class="btn btn-secondary btn-sm" type="button" data-toggle="collapse" data-target="#abstractCMIG" aria-expanded="false" aria-controls="collapseExample">
							Abstract
						</button>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12 mt-4">
						<div class="collapse" id="abstractCMIG">
							<div class="card card-body mb-3">
								Training deep ConvNets requires large labeled datasets. However, collecting pixel-level labels for medical image segmentation is very expensive and requires a high level of expertise. In addition, most existing segmentation masks provided by clinical experts focus on specific anatomical structures. In this paper, we propose a method dedicated to handle such partially labeled medical image datasets. We propose a strategy to identify pixels for which labels are correct, and to train Fully Convolutional Neural Networks with a multi-label loss adapted to this context. In addition, we introduce an iterative confidence self-training approach inspired by curriculum learning to relabel missing pixel labels, which relies on selecting the most confident prediction with a specifically designed confidence network that learns an uncertainty measure which is leveraged in our relabeling process. Our approach, INERRANT for Iterative coNfidencE Relabeling of paRtial ANnoTations, is thoroughly evaluated on two public datasets (TCAI and LITS), and one internal dataset with seven abdominal organ classes. We show that INERRANT robustly deals with partial labels, performing similarly to a model trained on all labels even for large missing label proportions. We also highlight the importance of our iterative learning scheme and the proposed confidence measure for optimal performance. Finally we show a practical use case where a limited number of completely labeled data are enriched by publicly available but partially labeled data.	
							</div>
						</div>
					</div>
				</div>
				
				
				<h5 class="mb-4">2019</h5>

				<div class="row">
					<div class="col-md-4">
						<img src="images/midl_2019_550x180.png" class="img-fluid" alt="MIDL Prior graphical absract">
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong>, Nicolas Thome and Luc Soler</i>.<br>
						"Biasing Deep ConvNets for Semantic Segmentation of Medical Images with a Prior-driven Prediction Function"<br>
						<a target="_blank" href='https://2019.midl.io/'>MIDL 2019</a>, extended abstract [<a href='/documents/Spatial_Prior_Extended_abstract_MIDL_2019.pdf'>PDF</a>]
						</p>
					</div>
				</div>

				

				<h5 class="mb-4">2018</h5>
				<div class="row">
					<div class="col-md-4">
						<img src="images/missing_annotation_illustration_550x180.png" class="img-fluid" alt="Illustration for the paper missing annotations">
					</div>
					<div class="col-md-7">
						<p>
						<i><strong>Olivier Petit</strong>, Nicolas Thome, Arnaud Charnoz, Alexandre Hostettler and Luc Soler</i>.<br>
						"Handling Missing Annotations for Semantic Segmentation with Deep ConvNets."<br>
						<a target="_blank" href='https://www.miccai2018.org/en/Default.asp?'>MICCAI 2018</a> workshop <a target="_blank" href='https://cs.adelaide.edu.au/~dlmia4/'>DLMIA</a> [<a href='/documents/handling_missing_annotations_DLMIA_MICCAI_2018.pdf'>PDF</a>]
						</p>
					</div>
					<div class="col-md-1">
						<button class="btn btn-secondary btn-sm" type="button" data-toggle="collapse" data-target="#abstractHMA" aria-expanded="false" aria-controls="collapseExample">
							Abstract
						</button>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12 mt-4">
						<div class="collapse" id="abstractHMA">
							<div class="card card-body mb-3">
								Annotation of medical images for semantic segmentation is a very time consuming and difficult task. Moreover, clinical experts often focus on specific anatomical structures and thus, produce partially annotated images. In this paper, we introduce SMILE, a new deep convolutional neural network which addresses the issue of learning with incomplete ground truth. SMILE aims to identify ambiguous labels in order to ignore them during training, and don't propagate incorrect or noisy information. A second contribution is SMILEr which uses SMILE as initialization for automatically relabeling missing annotations, using a curriculum strategy. Experiments on 3 organ classes (liver, stomach, pancreas) show the relevance of the proposed approach for semantic segmentation: with 70% of missing annotations, SMILEr performs similarly as a baseline trained with complete ground truth annotations.
							</div>
						</div>
					</div>
				</div>
				
			</div>

		</main>




		<div class="bg-light">
			<div class="container">
				<footer id="footer">
					<div class="inner">
						<ul class="nav">
							<li class="nav-item"><a class="nav-link disable">&copy; Olivier Petit</a></li>
						</ul>
					</div>
				</footer>
			</div>
		</div>



	</body>


	<foot>
		<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js" integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em" crossorigin="anonymous"></script>
	</foot>

</html>
